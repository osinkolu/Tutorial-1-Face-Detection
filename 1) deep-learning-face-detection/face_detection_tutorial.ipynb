{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Face Detection With OpenCV.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Good day folks, this tutorial is a deep learning tutuorial on facial detection made easy. First, we are going to perform facial detection on images, then we move to facial detection of streams like videos.\n",
    "        In this first part, it will suprise you to find out that opencv has very accurate face detectors embedded in the opencv library, though unknown to many, we are simply going to take a good advantage of it.Big thanks to Aleksandr Rybnikov, and his team for making this possible in opencv 3.3. The module supports google's Tensorflow, Facebook's Pytorch as well as the caffe framework. The caffe framework is hidden in the face_detector sub-directory of the dnn samples.\n",
    "        To convinently use the caffe models, you need two files:\n",
    "        1) The .prototxt file : this defines the model architecture, i.e the number of layers in the model.\n",
    "        2) The .caffemodel file contains the weights for the actual layers.\n",
    "These two files are required to train the model for deep learning. Although the prototxt files are included in the opencv github repo, the weight files are not quite easy to find. For ease, i'll include it in a folder \"face_detector\" to save you a lot of stress."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# How Does This Work??"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Opencv's deep learning face detector runs on the Single shot Detector(SSD) framework with a Resnet base network unlike other's familiar to you with the MobileNet as it's base network, of course you are well aware that a Resnet base network is more accurate than the MobileNet, although it is heavier and might lag behind the MobileNet in a seriously rigid speed test."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Let's Dive In."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "# first we import the necessary packages.\n",
    "import numpy as np\n",
    "import cv2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "face detector model is currently loading....\n"
     ]
    }
   ],
   "source": [
    "# load the face detector model from filepath\n",
    "print(\"face detector model is currently loading....\")\n",
    "# set address of prototxt file\n",
    "prototxtPath = \"face_detector/deploy.prototxt\"\n",
    "# set the address of the caffe_model\n",
    "weightspath = \"face_detector/res10_300x300_ssd_iter_140000.caffemodel\"\n",
    "\"\"\" NOTE THAT YOUR ADDRESS OF PROTOTXTPATH AND WEIGHTSPATH SHOULD BE SIMILAR TO THAT OF OpenCV's I.E ...../face_detector/ the file to load\"\"\"\n",
    "net = cv2.dnn.readNet(prototxtPath, weightspath)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we load the image and construct an input blob as well from it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load in your image\n",
    "image = cv2.imread(\"victor.jpg\")\n",
    "#pick image height and width\n",
    "(h, w) = image.shape[:2]\n",
    "# resize image to 300 by 300 pixels then normalize using the imagenet BGR mean values.\n",
    "blob = cv2.dnn.blobFromImage(cv2.resize(image, (300, 300)), 1.0, (300,300), (104.0,177.0,123.0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that the input blob is ready, we pass it into the network to obtain detections and give Predictions.\n",
    "\n",
    "just for your perusal, a blob simply means Binary Large Object. This simply is a region of an image with some properties constant such as intensity, color...etc. These region of pixels appear to be connected to each other."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "processor is computing object detections....\n"
     ]
    }
   ],
   "source": [
    "print(\"processor is computing object detections....\")\n",
    "#give input to network\n",
    "net.setInput(blob)\n",
    "#foward detections\n",
    "detections = net.forward()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We forwarded detections to get info about what we detected including the position of detection. Now we can easily draw a bounding box around it since we know it's position."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-1"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#loop over the detections:\n",
    "for i in range(0, detections.shape[2]):\n",
    "    #extract it's confidence of detection\n",
    "    confidence = detections[0,0,i,2]\n",
    "    # filter out the weak detections by ensuring confidence is greater than 0.5\n",
    "    if confidence > 0.5:\n",
    "        #if satisfied, we compute the x, y co-ordinates for the bounding box\n",
    "        box = detections[0,0,i,3:7] * np.array([w,h,w,h])\n",
    "        (startX, startY, endX, endY) = box.astype(\"int\")\n",
    "        \n",
    "        #Now draw the bounding box since you have correct dimensions\n",
    "        # Also, let's write the confidence of the model in text to be displayed.\n",
    "        text = str(confidence * 100) #this is the text\n",
    "       \n",
    "        #just in case the face is at the edge, we adjust where to write\n",
    "        y = startY -10 if startY -10 > 10 else startY + 10 \n",
    "        #tell cv to draw your rectangle and write your text as well, \n",
    "        cv2.rectangle(image, (startX, startY), (endX,endY), (0,225,0),2)\n",
    "        #specify your desired font too as well as other params, press shft + tab to see.\n",
    "        cv2.putText(image, text, (startX, y),cv2.FONT_ITALIC, 2.5,(0,225,0),2)\n",
    "\n",
    "# SHOW THE OUTPUT IMAGE\n",
    "\"\"\"image might be too big for your screen, just resize, else skip step\"\"\"\n",
    "# Original Image Height\n",
    "OIH = image.shape[0]\n",
    "# Original Image Width\n",
    "OIW = image.shape[1]\n",
    "#command cv to resize by 0.3 of original size\n",
    "image = cv2.resize(image,(round(OIW*0.3),round(OIH*0.3)))\n",
    "cv2.imshow(\"Output\", image)\n",
    "# JUST SO YOU CAN CLOSE THE FRAME WITH EASE.\n",
    "cv2.waitKey(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As a task for you, make this notebook more interesting by remoulding the code such that you can change the images easily, that is put the code in a function, then call out the function on an image to preview predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-1"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cv2.waitKey(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
